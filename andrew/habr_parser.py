import requests
from bs4 import BeautifulSoup
import re
import os
import time
from urllib.parse import urljoin
from typing import List, Dict, Optional
import json


class HabrArticleParser:
    def __init__(self, target_tags: List[str], max_articles_per_tag: int = 5):
        self.base_url = "https://habr.com"
        self.target_tags = target_tags
        self.max_articles_per_tag = max_articles_per_tag
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        }
        self.session = requests.Session()
        self.session.headers.update(self.headers)
        
        os.makedirs("habr_articles", exist_ok=True)
    
    def get_articles_by_tag(self, tag: str) -> List[str]:
        """–ü–æ–∏—Å–∫ —Å—Ç–∞—Ç–µ–π –ø–æ —Ç–µ–≥—É"""
        articles_urls = []
        
        print(f"\nüîç –ü–æ–∏—Å–∫ —Å—Ç–∞—Ç–µ–π –ø–æ —Ç–µ–≥—É: {tag}")
        
        try:
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ç–µ–≥ –¥–ª—è URL
            tag_formatted = tag.lower().replace(' ', '_').replace('-', '_')
            
            # –î–ª—è –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö —Ç–µ–≥–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ö–∞–±—ã
            hub_mapping = {
                'python': 'python',
                '–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç': 'machine_learning',
                'javascript': 'javascript',
                '–º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ': 'machine_learning',
                'ai': 'artificial_intelligence',
            }
            
            hub_name = hub_mapping.get(tag.lower(), tag_formatted)
            url = f"{self.base_url}/ru/hub/{hub_name}/"
            
            print(f"  –ò—Å–ø–æ–ª—å–∑—É–µ–º URL: {url}")
            
            response = self.session.get(url, timeout=15)
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # –ò—â–µ–º —Å—Ç–∞—Ç—å–∏
            article_elements = soup.find_all('article', class_='tm-articles-list__item')
            
            if not article_elements:
                article_elements = soup.find_all('h2', class_='tm-title')
            
            for i, article in enumerate(article_elements[:self.max_articles_per_tag]):
                if len(articles_urls) >= self.max_articles_per_tag:
                    break
                
                # –ò—â–µ–º —Å—Å—ã–ª–∫—É
                link_element = None
                if hasattr(article, 'find'):
                    link_element = article.find('a', class_='tm-title__link')
                
                if not link_element and hasattr(article, 'parent'):
                    link_element = article.parent.find('a')
                
                if link_element and link_element.get('href'):
                    href = link_element['href']
                    if '/articles/' in href or '/post/' in href:
                        full_url = urljoin(self.base_url, href)
                        full_url = full_url.split('?')[0]
                        
                        if full_url not in articles_urls:
                            articles_urls.append(full_url)
                            print(f"  ‚úÖ –ù–∞–π–¥–µ–Ω–∞ —Å—Ç–∞—Ç—å—è {i+1}")
            
            time.sleep(1)
            
        except Exception as e:
            print(f"  ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ: {e}")
        
        print(f"üìä –ù–∞–π–¥–µ–Ω–æ —Å—Ç–∞—Ç–µ–π: {len(articles_urls)}")
        return articles_urls
    
    def extract_article_text(self, soup: BeautifulSoup) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ —Å—Ç–∞—Ç—å–∏"""
        try:
            # –ò—â–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç
            article_body = soup.find('div', id='post-content-body')
            
            if not article_body:
                article_body = soup.find('div', class_='tm-article-body')
            
            if not article_body:
                article_body = soup.find('article')
            
            if not article_body:
                return "–¢–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω"
            
            # –£–¥–∞–ª—è–µ–º –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –∏ —Ä–µ–∫–ª–∞–º—É
            for element in article_body.find_all(class_=lambda x: x and any(
                word in x.lower() for word in ['comment', 'discuss', 'recommended', 'adv', 'ad']
            )):
                element.decompose()
            
            # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç
            text = article_body.get_text(separator='\n', strip=True)
            text = re.sub(r'\n{3,}', '\n\n', text)
            
            return text
            
        except Exception as e:
            print(f"  ‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞: {e}")
            return "–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ —Ç–µ–∫—Å—Ç–∞"
    
    def parse_article(self, url: str) -> Optional[Dict]:
        """–ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç–∞—Ç—å–∏ —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º –ø–æ–∏—Å–∫–æ–º —Ç–µ–≥–æ–≤"""
        print(f"\nüìñ –ü–∞—Ä—Å–∏–Ω–≥: {url}")
        
        try:
            response = self.session.get(url, timeout=15)
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # 1. –ê–≤—Ç–æ—Ä
            author = "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∞–≤—Ç–æ—Ä"
            author_elem = soup.find('a', class_='tm-user-info__username')
            if author_elem:
                author = author_elem.text.strip().split('¬∑')[0].strip()
            
            print(f"  üë§ –ê–≤—Ç–æ—Ä: {author}")
            
            # 2. –ó–∞–≥–æ–ª–æ–≤–æ–∫
            title = "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è"
            title_elem = soup.find('h1', class_='tm-title')
            if title_elem:
                title = title_elem.find('span').text.strip() if title_elem.find('span') else title_elem.text.strip()
            
            print(f"  üìù –ó–∞–≥–æ–ª–æ–≤–æ–∫: {title[:80]}...")
            
            # 3. –¢–ï–ì–ò - –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ô –ü–û–ò–°–ö
            tags = []
            
            # –°–ø–æ—Å–æ–± 1: –û—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä —Ç–µ–≥–æ–≤ –Ω–∞ –•–∞–±—Ä–µ
            tags_container = soup.find('div', class_='tm-publication-hubs')
            
            # –°–ø–æ—Å–æ–± 2: –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫
            if not tags_container:
                tags_container = soup.find('div', class_='tm-article-presenter__meta-list')
            
            # –°–ø–æ—Å–æ–± 3: –ü–æ–∏—Å–∫ –≤—Å–µ—Ö —Å—Å—ã–ª–æ–∫ —Å –∫–ª–∞—Å—Å–æ–º hub-link
            if not tags_container:
                hub_links = soup.find_all('a', class_='tm-hubs-list__link')
                tags = [link.text.strip() for link in hub_links if link.text.strip()]
            
            # –°–ø–æ—Å–æ–± 4: –ü–æ–∏—Å–∫ –≤ meta-—Ç–µ–≥–∞—Ö
            if not tags:
                meta_tags = soup.find_all('meta', {'name': 'keywords'})
                if meta_tags:
                    keywords = meta_tags[0].get('content', '')
                    tags = [k.strip() for k in keywords.split(',') if k.strip()]
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä
            if tags_container and not tags:
                # –ò—â–µ–º —Ç–µ–≥–∏ –≤–Ω—É—Ç—Ä–∏ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞
                tag_links = tags_container.find_all('a')
                for link in tag_links:
                    tag_text = link.text.strip()
                    if tag_text and tag_text not in ['...', '']:
                        tags.append(tag_text)
            
            print(f"  üè∑Ô∏è  –ù–∞–π–¥–µ–Ω–æ —Ç–µ–≥–æ–≤: {len(tags)}")
            if tags:
                print(f"  üè∑Ô∏è  –¢–µ–≥–∏: {', '.join(tags[:5])}")
            
            # 4. –¢–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏
            content = self.extract_article_text(soup)
            print(f"  üìÑ –î–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞: {len(content)} —Å–∏–º–≤–æ–ª–æ–≤")
            
            return {
                'author': author,
                'url': url,
                'title': title,
                'tags': tags,
                'content': content
            }
            
        except Exception as e:
            print(f"  ‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: {e}")
            return None
    
    def save_article_to_txt(self, article_data: Dict, tag: str, index: int):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å—Ç–∞—Ç—å–∏"""
        try:
            safe_title = re.sub(r'[^\w\s-]', '', article_data['title'])
            safe_title = re.sub(r'[-\s]+', '_', safe_title)[:50]
            
            filename = f"habr_articles/{tag}_{index:03d}_{safe_title}.txt"
            
            with open(filename, 'w', encoding='utf-8') as f:
                f.write(f"{article_data['author']}\n")
                f.write(f"{article_data['url']}\n")
                f.write(f"{article_data['title']}\n")
                
                # –¢–ï–ì–ò - –≤—Å–µ–≥–¥–∞ –∑–∞–ø–∏—Å—ã–≤–∞–µ–º, –¥–∞–∂–µ –µ—Å–ª–∏ –ø—É—Å—Ç—ã–µ
                tags_str = ', '.join(article_data['tags']) if article_data['tags'] else '–ù–µ—Ç —Ç–µ–≥–æ–≤'
                f.write(f"{tags_str}\n\n")
                
                f.write(article_data['content'])
            
            print(f"  üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {filename}")
            print(f"  üìù –¢–µ–≥–∏ –≤ —Ñ–∞–π–ª–µ: {tags_str}")
            return True
            
        except Exception as e:
            print(f"  ‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {e}")
            return False
    
    def run(self):
        """–ó–∞–ø—É—Å–∫ –ø–∞—Ä—Å–µ—Ä–∞"""
        print("=" * 60)
        print("–ü–ê–†–°–ï–† HABR –° –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ú–ò –¢–ï–ì–ê–ú–ò")
        print("=" * 60)
        
        for tag in self.target_tags:
            print(f"\n{'='*60}")
            print(f"–¢–ï–ì: {tag.upper()}")
            print(f"{'='*60}")
            
            articles = self.get_articles_by_tag(tag)
            
            for i, url in enumerate(articles, 1):
                print(f"\n--- –°—Ç–∞—Ç—å—è {i}/{len(articles)} ---")
                article_data = self.parse_article(url)
                
                if article_data:
                    self.save_article_to_txt(article_data, tag, i)
                
                time.sleep(1)
        
        print(f"\n{'='*60}")
        print("–ì–û–¢–û–í–û! –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Ñ–∞–π–ª—ã –≤ –ø–∞–ø–∫–µ habr_articles/")
        print(f"{'='*60}")


def test_tags_on_example():
    """–¢–µ—Å—Ç–æ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø–æ–∏—Å–∫–∞ —Ç–µ–≥–æ–≤ –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π —Å—Ç–∞—Ç—å–µ"""
    test_url = "https://habr.com/ru/companies/nix/articles/342904/"
    
    print("üîç –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø–æ–∏—Å–∫ —Ç–µ–≥–æ–≤ –Ω–∞ –ø—Ä–∏–º–µ—Ä–µ —Å—Ç–∞—Ç—å–∏...")
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    }
    
    response = requests.get(test_url, headers=headers)
    soup = BeautifulSoup(response.text, 'html.parser')
    
    print("\n1. –ü–æ–∏—Å–∫ –ø–æ class='tm-publication-hubs':")
    hubs = soup.find_all('div', class_='tm-publication-hubs')
    print(f"   –ù–∞–π–¥–µ–Ω–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤: {len(hubs)}")
    for i, hub in enumerate(hubs[:3]):
        print(f"   –≠–ª–µ–º–µ–Ω—Ç {i+1}: {hub}")
        if hub.find('a'):
            print(f"   –¢–µ–≥–∏ –≤–Ω—É—Ç—Ä–∏: {[a.text.strip() for a in hub.find_all('a')]}")
    
    print("\n2. –ü–æ–∏—Å–∫ –ø–æ class='tm-article-presenter__meta-list':")
    meta_list = soup.find_all('div', class_='tm-article-presenter__meta-list')
    print(f"   –ù–∞–π–¥–µ–Ω–æ: {len(meta_list)}")
    
    print("\n3. –ü–æ–∏—Å–∫ –≤—Å–µ—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤ —Å 'hub' –≤ –∫–ª–∞—Å—Å–µ:")
    all_hub_elements = soup.find_all(class_=lambda x: x and 'hub' in x.lower())
    print(f"   –ù–∞–π–¥–µ–Ω–æ: {len(all_hub_elements)}")
    for elem in all_hub_elements[:5]:
        print(f"   –ö–ª–∞—Å—Å: {elem.get('class')}, –¢–µ–∫—Å—Ç: {elem.text[:50]}...")
    
    print("\n4. –ü–æ–∏—Å–∫ –≤—Å–µ—Ö —Å—Å—ã–ª–æ–∫ —Å —Ç–µ–≥–∞–º–∏:")
    all_links = soup.find_all('a')
    tag_links = []
    for link in all_links:
        href = link.get('href', '')
        text = link.text.strip()
        if '/hub/' in href and text:
            tag_links.append((text, href))
    
    print(f"   –ù–∞–π–¥–µ–Ω–æ —Å—Å—ã–ª–æ–∫ –Ω–∞ —Ö–∞–±—ã: {len(tag_links)}")
    for text, href in tag_links[:10]:
        print(f"   –¢–µ–≥: '{text}', –°—Å—ã–ª–∫–∞: {href}")


def main():
    # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä—É–π—Ç–µ
    #test_tags_on_example()
    
    # –ó–∞—Ç–µ–º –∑–∞–ø—É—Å—Ç–∏—Ç–µ –ø–∞—Ä—Å–µ—Ä
    target_tags = ['Python', '–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç', 'JavaScript']
    parser = HabrArticleParser(target_tags, max_articles_per_tag=20)
    parser.run()


if __name__ == "__main__":
    main()